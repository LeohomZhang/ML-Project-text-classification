{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "353e6847-066e-4239-bc10-0a81a67d638c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, confusion_matrix, roc_curve, auc, accuracy_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "# display confusion matrix:\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "#from tune_sklearn import TuneSearchCV\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import imblearn\n",
    "import lightgbm as lgb\n",
    "targs = ['target_question','target_discarded','target_subsection','target_section']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "054fe019-193b-4f4a-a9cd-913a201d5765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 16.3 s\n",
      "Wall time: 6.62 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "file_path = 'GT_IPI.parquet'\n",
    "df = pd.read_parquet(file_path, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c7b1eed-f2bb-43ac-bfd8-83dfba868dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['css_pk']=='3825451'].to_csv('bad guy.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e59b0a6-ddd7-491a-b73f-e0f43c081ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_df = pd.read_csv('unique_indexes.csv')\n",
    "indices = indices_df['unique_index'].tolist()\n",
    "filtered_df = df[df.index.isin(indices)]\n",
    "\n",
    "# If you want to reset the index after filtering (optional)\n",
    "filtered_df.reset_index(drop=True, inplace=True)\n",
    "df = filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2408ae93-b78d-4448-a898-902bf33d8b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wyatt\\AppData\\Local\\Temp\\ipykernel_4724\\793476661.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  target_df['Percentage'] = (target_df['Count'] / total_terms) * 100\n",
      "C:\\Users\\wyatt\\AppData\\Local\\Temp\\ipykernel_4724\\793476661.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  target_df['Percentage'] = (target_df['Count'] / total_terms) * 100\n",
      "C:\\Users\\wyatt\\AppData\\Local\\Temp\\ipykernel_4724\\793476661.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  target_df['Percentage'] = (target_df['Count'] / total_terms) * 100\n",
      "C:\\Users\\wyatt\\AppData\\Local\\Temp\\ipykernel_4724\\793476661.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  target_df['Percentage'] = (target_df['Count'] / total_terms) * 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique 'style' values in the top 15 per target:\n",
      "['default' 'listparagraph' 'bodytext' 'tableparagraph' 'normal' 'heading2'\n",
      " 'toc2' 'toc1' 'heading1' 'nospacing' 'heading3' 'normalweb' 'tabletext'\n",
      " 'toc3' 'listnumber' 'question' 'dvquestion' 'paragraphedeliste'\n",
      " 'esisquestion' 'title' 'heading4' 'heading10' 'paragraph' 'titre1'\n",
      " 'dvsubcategory' 'rfpheading3' 'himheading4bodybold' 'heading20']\n"
     ]
    }
   ],
   "source": [
    "bool_column = df.apply({0,1}.issuperset)\n",
    "bool_column_list = bool_column[bool_column == True].index.to_list()\n",
    "style_counts_by_target = df.groupby('target')['style'].value_counts()\n",
    "# too many style value in the column style\n",
    "style_counts_by_target = df.groupby('target')['style'].value_counts()\n",
    "\n",
    "# Convert the result to a DataFrame\n",
    "style_counts_by_target_df = style_counts_by_target.to_frame(name='Count').reset_index()\n",
    "\n",
    "# Initialize an empty list to hold the top 10 terms for each target\n",
    "top_15_per_target = []\n",
    "\n",
    "# Calculate the percentage representation and get the top 15 terms for each 'target'\n",
    "for target in style_counts_by_target_df['target'].unique():\n",
    "    # Filter the DataFrame for the current target\n",
    "    target_df = style_counts_by_target_df[style_counts_by_target_df['target'] == target]\n",
    "    \n",
    "    # Calculate the total number of terms for the current target\n",
    "    total_terms = target_df['Count'].sum()\n",
    "    \n",
    "    # Calculate the percentage representation of each term\n",
    "    target_df['Percentage'] = (target_df['Count'] / total_terms) * 100\n",
    "    \n",
    "    # Get the top 15 terms\n",
    "    top_15 = target_df.head(15)\n",
    "    \n",
    "    # Append the result to the list\n",
    "    top_15_per_target.append(top_15)\n",
    "\n",
    "# Concatenate all top 10 DataFrames into a single DataFrame\n",
    "combined_top_15 = pd.concat(top_15_per_target)\n",
    "\n",
    "# Display the unique 'style' values in the combined DataFrame\n",
    "unique_styles = combined_top_15['style'].unique()\n",
    "## manually remove title from the list as it is duplicated\n",
    "#unique_styles = np.delete(unique_styles, np.where(unique_styles == 'title'))\n",
    "print(\"Unique 'style' values in the top 15 per target:\")\n",
    "print(unique_styles)\n",
    "# Transform the 'style' column to 'None' if the current value isn't found in unique_styles\n",
    "df['style'] = df['style'].apply(lambda x: x if x in unique_styles else 'None')\n",
    "# Keep a copy of the original 'target' column\n",
    "original_target = df['target'].copy()\n",
    "\n",
    "# Apply one-hot encoding to the specified columns\n",
    "#df_encoded = pd.get_dummies(df, columns=['target', 'begins_with', 'style'], prefix=['target', 'begins_with', 'style'])\n",
    "df_encoded = pd.get_dummies(df,drop_first = True\n",
    "                            , columns=['begins_with','para_foll_depth_ind','para_foll_size_ind', 'para_prec_depth_ind', 'para_prec_size_ind', 'style'] + bool_column_list\\\n",
    "                            , prefix=['begins_with','para_foll_depth_ind','para_foll_size_ind', 'para_prec_depth_ind', 'para_prec_size_ind', 'style'] + bool_column_list)\n",
    "df_encoded = pd.get_dummies(df_encoded\n",
    "                            , columns=['target']\\\n",
    "                            , prefix=['target'] )\n",
    "\n",
    "# Add the original 'target' column back to the encoded DataFrame\n",
    "df_encoded['target'] = original_target\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf6b5152-d662-4ef0-ae1c-7aaa4acf8bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_with_less_than2_csspk = [\n",
    "    x for x in df_encoded['customer_pk'].unique() \n",
    "    if len(df_encoded[df_encoded['customer_pk'] == x]['css_pk'].unique()) < 2\n",
    "]\n",
    "\n",
    "filtered_df = df_encoded[~df_encoded['customer_pk'].isin(customers_with_less_than2_csspk)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d94408b1-a597-43ea-b182-387a9db68721",
   "metadata": {},
   "outputs": [],
   "source": [
    "#array(['question', 'discarded', 'subsection', 'section'],\n",
    "exclude = ['lang_num_sents','css_pk','customer_pk','html_pk','id']\n",
    "#top_15_columns\n",
    "top_15_columns = []\n",
    "columns_to_exclude = exclude + top_15_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32ddf0ec-5f42-4508-800c-dd13b41199fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train DataFrame length for r=1: 2438929\n",
      "Test DataFrame length for r=1: 680281\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "# Lists to store the different train and test dataframes\n",
    "r_state = [1, 2, 3, 4, 5]\n",
    "train_df_dict = dict()\n",
    "test_df_dict = dict()\n",
    "\n",
    "# Generate 5 different train/test splits\n",
    "for r in r_state:\n",
    "    gss = GroupShuffleSplit(n_splits=1, train_size=0.8, random_state=10*r)\n",
    "    train_list = []\n",
    "    test_list = []\n",
    "\n",
    "    # Perform train-test split within each group\n",
    "    for i in filtered_df['customer_pk'].unique():\n",
    "        X = filtered_df.loc[filtered_df['customer_pk'] == i]\n",
    "        train_index, test_index = next(gss.split(X, groups=X['css_pk']))\n",
    "        \n",
    "        # Append the subset data to the respective list\n",
    "        train_list.append(X.iloc[train_index])\n",
    "        test_list.append(X.iloc[test_index])\n",
    "\n",
    "    # Concatenate the lists to create final train and test DataFrames\n",
    "    train_df_dict[r] = pd.concat(train_list, ignore_index=True)\n",
    "    test_df_dict[r] = pd.concat(test_list, ignore_index=True)\n",
    "\n",
    "# Check the length of one of the train and test sets\n",
    "print(f\"Train DataFrame length for r=1: {len(train_df_dict[1])}\")\n",
    "print(f\"Test DataFrame length for r=1: {len(test_df_dict[1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abb3b061-59e8-462a-a09c-d7efaaeeecd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "def multi_pred_stats(predicts_list):\n",
    "    # Extract true labels from each prediction list\n",
    "    true_question = [subarray[1] for subarray in predicts_list[0]]\n",
    "    true_discarded = [subarray[1] for subarray in predicts_list[1]]\n",
    "    true_subsection = [subarray[1] for subarray in predicts_list[2]]\n",
    "    true_section = [subarray[1] for subarray in predicts_list[3]]\n",
    "    \n",
    "    # Stack arrays and determine the maximum indices\n",
    "    stacked_arrays = np.vstack((true_question, true_discarded, true_section, true_subsection))\n",
    "    label_names = ['question', 'discarded', 'section', 'subsection']\n",
    "    max_indices = np.argmax(stacked_arrays, axis=0)\n",
    "    result = [label_names[i] for i in max_indices]\n",
    "    \n",
    "    # Calculate classification report for the combined prediction\n",
    "    y_true_overall = test_df['target']\n",
    "    y_pred_overall = result\n",
    "    overall_class_report = classification_report(y_true_overall, y_pred_overall)\n",
    "    print(overall_class_report)\n",
    "    return overall_class_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "32f94fbe-2fe1-4b4d-98af-c554c9a7f3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "# Initialize lists to store reports and features\n",
    "def train_test(target, sampling_tech='none', lgb_params=None):\n",
    "    if lgb_params is None:\n",
    "        lgb_params = {}\n",
    "\n",
    "    X_train = train_df.drop(columns=columns_to_exclude + ['target'] + targs)\n",
    "    y_train = train_df[f'target_{target}']\n",
    "    X_test = test_df.drop(columns=columns_to_exclude + ['target'] + targs)\n",
    "    y_test = test_df[f'target_{target}']\n",
    "\n",
    "    if sampling_tech == 'under':\n",
    "        rus = imblearn.under_sampling.RandomUnderSampler(random_state=420)\n",
    "    elif sampling_tech == 'over':\n",
    "        rus = imblearn.over_sampling.RandomOverSampler(random_state=420)\n",
    "    elif sampling_tech == 'smote':\n",
    "        rus = SMOTE(random_state=420)\n",
    "    elif sampling_tech == 'adasyn':\n",
    "        rus = ADASYN(random_state=420)\n",
    "    \n",
    "    if sampling_tech in ['over', 'under', 'smote', 'adasyn']:\n",
    "        X_train_resampled, y_train_resampled = rus.fit_resample(X_train, y_train)\n",
    "        X_train = X_train_resampled\n",
    "        y_train = y_train_resampled\n",
    "\n",
    "    model = lgb.LGBMClassifier(**lgb_params)\n",
    "    model.fit(X_train, y_train)\n",
    "    predicts = model.predict_proba(X_test)\n",
    "\n",
    "    return predicts, model\n",
    "\n",
    "params_s = {'bagging_fraction': 0.3633904999185218, 'feature_fraction': 1.0, 'lambda_l1': 3.574560893629339, 'lambda_l2': 9.818056164657532, 'learning_rate': 0.4851255385542588, 'max_depth': 12, 'min_child_samples': 81, 'min_child_weight': 12.168209299428229, 'min_split_gain': 0.05264829965012845, 'n_estimators': 506, 'num_leaves': 16}\n",
    "params_d = {'bagging_fraction': 0.6243816730492119, 'feature_fraction': 0.45028179026459975, 'lambda_l1': 1.1971042837442278, 'lambda_l2': 4.297245941070125, 'learning_rate': 0.5981568321835795, 'max_depth': 9, 'min_child_samples': 83, 'min_child_weight': 12.608881674364874, 'min_split_gain': 0.5222560899891926, 'n_estimators': 506, 'num_leaves': 52}\n",
    "params_q = {'bagging_fraction': 0.9702107536403743, 'feature_fraction': 0.8341182143924175, 'lambda_l1': 2.1233911067827616, 'lambda_l2': 1.8182496720710062, 'learning_rate': 0.15488956278421273, 'max_depth': 8, 'min_child_samples': 57, 'min_child_weight': 8.644580922655893, 'min_split_gain': 0.2912291401980419, 'n_estimators': 612, 'num_leaves': 14}\n",
    "params_ss = {'bagging_fraction': 0.09154053332582732, 'feature_fraction': 0.8620892013062822, 'lambda_l1': 2.979058792848589, 'lambda_l2': 6.228517664360759, 'learning_rate': 0.36057259624318583, 'max_depth': 15, 'min_child_samples': 86, 'min_child_weight': 11.202870028573141, 'min_split_gain': 0.456877355938568, 'n_estimators': 501, 'num_leaves': 20}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce7896aa-9aeb-49f2-b1d8-579120874722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run 1\n",
      "[LightGBM] [Info] Number of positive: 1135215, number of negative: 1303714\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.241390 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8503\n",
      "[LightGBM] [Info] Number of data points in the train set: 2438929, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.465456 -> initscore=-0.138395\n",
      "[LightGBM] [Info] Start training from score -0.138395\n",
      "[LightGBM] [Info] Number of positive: 1221432, number of negative: 1217497\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.231718 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8503\n",
      "[LightGBM] [Info] Number of data points in the train set: 2438929, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500807 -> initscore=0.003227\n",
      "[LightGBM] [Info] Start training from score 0.003227\n",
      "[LightGBM] [Info] Number of positive: 35164, number of negative: 2403765\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.242430 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8503\n",
      "[LightGBM] [Info] Number of data points in the train set: 2438929, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014418 -> initscore=-4.224769\n",
      "[LightGBM] [Info] Start training from score -4.224769\n",
      "[LightGBM] [Info] Number of positive: 47118, number of negative: 2391811\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.232423 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8503\n",
      "[LightGBM] [Info] Number of data points in the train set: 2438929, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.019319 -> initscore=-3.927151\n",
      "[LightGBM] [Info] Start training from score -3.927151\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   discarded       0.78      0.82      0.80    343880\n",
      "    question       0.80      0.79      0.80    313673\n",
      "     section       0.53      0.20      0.29     12516\n",
      "  subsection       0.56      0.06      0.11     10212\n",
      "\n",
      "    accuracy                           0.79    680281\n",
      "   macro avg       0.67      0.47      0.50    680281\n",
      "weighted avg       0.78      0.79      0.78    680281\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   discarded       0.78      0.82      0.80    343880\n",
      "    question       0.80      0.79      0.80    313673\n",
      "     section       0.53      0.20      0.29     12516\n",
      "  subsection       0.56      0.06      0.11     10212\n",
      "\n",
      "    accuracy                           0.79    680281\n",
      "   macro avg       0.67      0.47      0.50    680281\n",
      "weighted avg       0.78      0.79      0.78    680281\n",
      "\n",
      "run 2\n",
      "[LightGBM] [Info] Number of positive: 1138595, number of negative: 1318124\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.242849 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8365\n",
      "[LightGBM] [Info] Number of data points in the train set: 2456719, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.463462 -> initscore=-0.146414\n",
      "[LightGBM] [Info] Start training from score -0.146414\n",
      "[LightGBM] [Info] Number of positive: 1235033, number of negative: 1221686\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.243944 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8365\n",
      "[LightGBM] [Info] Number of data points in the train set: 2456719, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502716 -> initscore=0.010866\n",
      "[LightGBM] [Info] Start training from score 0.010866\n",
      "[LightGBM] [Info] Number of positive: 36202, number of negative: 2420517\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.251736 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8365\n",
      "[LightGBM] [Info] Number of data points in the train set: 2456719, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014736 -> initscore=-4.202622\n",
      "[LightGBM] [Info] Start training from score -4.202622\n",
      "[LightGBM] [Info] Number of positive: 46889, number of negative: 2409830\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.221677 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8365\n",
      "[LightGBM] [Info] Number of data points in the train set: 2456719, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.019086 -> initscore=-3.939528\n",
      "[LightGBM] [Info] Start training from score -3.939528\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   discarded       0.78      0.84      0.81    330279\n",
      "    question       0.82      0.80      0.81    310293\n",
      "     section       0.54      0.19      0.28     12745\n",
      "  subsection       0.49      0.05      0.09      9174\n",
      "\n",
      "    accuracy                           0.80    662491\n",
      "   macro avg       0.66      0.47      0.50    662491\n",
      "weighted avg       0.79      0.80      0.79    662491\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   discarded       0.78      0.84      0.81    330279\n",
      "    question       0.82      0.80      0.81    310293\n",
      "     section       0.54      0.19      0.28     12745\n",
      "  subsection       0.49      0.05      0.09      9174\n",
      "\n",
      "    accuracy                           0.80    662491\n",
      "   macro avg       0.66      0.47      0.50    662491\n",
      "weighted avg       0.79      0.80      0.79    662491\n",
      "\n",
      "run 3\n",
      "[LightGBM] [Info] Number of positive: 1133648, number of negative: 1314791\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.231750 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8482\n",
      "[LightGBM] [Info] Number of data points in the train set: 2448439, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.463008 -> initscore=-0.148237\n",
      "[LightGBM] [Info] Start training from score -0.148237\n",
      "[LightGBM] [Info] Number of positive: 1232274, number of negative: 1216165\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.236250 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8482\n",
      "[LightGBM] [Info] Number of data points in the train set: 2448439, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503290 -> initscore=0.013159\n",
      "[LightGBM] [Info] Start training from score 0.013159\n",
      "[LightGBM] [Info] Number of positive: 35520, number of negative: 2412919\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.238772 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8482\n",
      "[LightGBM] [Info] Number of data points in the train set: 2448439, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014507 -> initscore=-4.218497\n",
      "[LightGBM] [Info] Start training from score -4.218497\n",
      "[LightGBM] [Info] Number of positive: 46997, number of negative: 2401442\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.226726 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8482\n",
      "[LightGBM] [Info] Number of data points in the train set: 2448439, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.019195 -> initscore=-3.933741\n",
      "[LightGBM] [Info] Start training from score -3.933741\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   discarded       0.77      0.83      0.80    333038\n",
      "    question       0.81      0.80      0.80    315240\n",
      "     section       0.54      0.20      0.29     12637\n",
      "  subsection       0.49      0.06      0.11      9856\n",
      "\n",
      "    accuracy                           0.79    670771\n",
      "   macro avg       0.65      0.47      0.50    670771\n",
      "weighted avg       0.78      0.79      0.78    670771\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   discarded       0.77      0.83      0.80    333038\n",
      "    question       0.81      0.80      0.80    315240\n",
      "     section       0.54      0.20      0.29     12637\n",
      "  subsection       0.49      0.06      0.11      9856\n",
      "\n",
      "    accuracy                           0.79    670771\n",
      "   macro avg       0.65      0.47      0.50    670771\n",
      "weighted avg       0.78      0.79      0.78    670771\n",
      "\n",
      "run 4\n",
      "[LightGBM] [Info] Number of positive: 1141432, number of negative: 1303286\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.226968 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8427\n",
      "[LightGBM] [Info] Number of data points in the train set: 2444718, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.466897 -> initscore=-0.132605\n",
      "[LightGBM] [Info] Start training from score -0.132605\n",
      "[LightGBM] [Info] Number of positive: 1220374, number of negative: 1224344\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.240567 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8427\n",
      "[LightGBM] [Info] Number of data points in the train set: 2444718, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499188 -> initscore=-0.003248\n",
      "[LightGBM] [Info] Start training from score -0.003248\n",
      "[LightGBM] [Info] Number of positive: 35662, number of negative: 2409056\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.243972 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8427\n",
      "[LightGBM] [Info] Number of data points in the train set: 2444718, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014587 -> initscore=-4.212905\n",
      "[LightGBM] [Info] Start training from score -4.212905\n",
      "[LightGBM] [Info] Number of positive: 47250, number of negative: 2397468\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.231521 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8427\n",
      "[LightGBM] [Info] Number of data points in the train set: 2444718, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.019327 -> initscore=-3.926716\n",
      "[LightGBM] [Info] Start training from score -3.926716\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   discarded       0.79      0.84      0.81    344938\n",
      "    question       0.81      0.80      0.80    307456\n",
      "     section       0.53      0.20      0.29     12384\n",
      "  subsection       0.47      0.07      0.11      9714\n",
      "\n",
      "    accuracy                           0.80    674492\n",
      "   macro avg       0.65      0.47      0.51    674492\n",
      "weighted avg       0.79      0.80      0.79    674492\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   discarded       0.79      0.84      0.81    344938\n",
      "    question       0.81      0.80      0.80    307456\n",
      "     section       0.53      0.20      0.29     12384\n",
      "  subsection       0.47      0.07      0.11      9714\n",
      "\n",
      "    accuracy                           0.80    674492\n",
      "   macro avg       0.65      0.47      0.51    674492\n",
      "weighted avg       0.79      0.80      0.79    674492\n",
      "\n",
      "run 5\n",
      "[LightGBM] [Info] Number of positive: 1130426, number of negative: 1309641\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.195217 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8463\n",
      "[LightGBM] [Info] Number of data points in the train set: 2440067, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.463277 -> initscore=-0.147159\n",
      "[LightGBM] [Info] Start training from score -0.147159\n",
      "[LightGBM] [Info] Number of positive: 1227270, number of negative: 1212797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.223893 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8463\n",
      "[LightGBM] [Info] Number of data points in the train set: 2440067, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502966 -> initscore=0.011863\n",
      "[LightGBM] [Info] Start training from score 0.011863\n",
      "[LightGBM] [Info] Number of positive: 35898, number of negative: 2404169\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.274951 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8463\n",
      "[LightGBM] [Info] Number of data points in the train set: 2440067, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014712 -> initscore=-4.204278\n",
      "[LightGBM] [Info] Start training from score -4.204278\n",
      "[LightGBM] [Info] Number of positive: 46473, number of negative: 2393594\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.232098 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8463\n",
      "[LightGBM] [Info] Number of data points in the train set: 2440067, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.019046 -> initscore=-3.941680\n",
      "[LightGBM] [Info] Start training from score -3.941680\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   discarded       0.78      0.84      0.81    338042\n",
      "    question       0.83      0.80      0.81    318462\n",
      "     section       0.56      0.20      0.29     13161\n",
      "  subsection       0.45      0.06      0.10      9478\n",
      "\n",
      "    accuracy                           0.80    679143\n",
      "   macro avg       0.66      0.47      0.50    679143\n",
      "weighted avg       0.79      0.80      0.79    679143\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   discarded       0.78      0.84      0.81    338042\n",
      "    question       0.83      0.80      0.81    318462\n",
      "     section       0.56      0.20      0.29     13161\n",
      "  subsection       0.45      0.06      0.10      9478\n",
      "\n",
      "    accuracy                           0.80    679143\n",
      "   macro avg       0.66      0.47      0.50    679143\n",
      "weighted avg       0.79      0.80      0.79    679143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reports1 = []\n",
    "\n",
    "for r in r_state:\n",
    "    print(f'run {r}')\n",
    "    train_df = train_df_dict[r]\n",
    "    test_df = test_df_dict[r]\n",
    "    z = multi_pred_stats([train_test('question')[0],train_test('discarded')[0],train_test('subsection')[0],train_test('section')[0]])\n",
    "    reports1.append(z)\n",
    "    print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a7b3f062-76a0-48b4-8fa2-ec98049fe83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   discarded       0.78      0.82      0.80    343880\n",
      "    question       0.80      0.79      0.80    313673\n",
      "     section       0.53      0.20      0.29     12516\n",
      "  subsection       0.56      0.06      0.11     10212\n",
      "\n",
      "    accuracy                           0.79    680281\n",
      "   macro avg       0.67      0.47      0.50    680281\n",
      "weighted avg       0.78      0.79      0.78    680281\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   discarded       0.78      0.84      0.81    330279\n",
      "    question       0.82      0.80      0.81    310293\n",
      "     section       0.54      0.19      0.28     12745\n",
      "  subsection       0.49      0.05      0.09      9174\n",
      "\n",
      "    accuracy                           0.80    662491\n",
      "   macro avg       0.66      0.47      0.50    662491\n",
      "weighted avg       0.79      0.80      0.79    662491\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   discarded       0.77      0.83      0.80    333038\n",
      "    question       0.81      0.80      0.80    315240\n",
      "     section       0.54      0.20      0.29     12637\n",
      "  subsection       0.49      0.06      0.11      9856\n",
      "\n",
      "    accuracy                           0.79    670771\n",
      "   macro avg       0.65      0.47      0.50    670771\n",
      "weighted avg       0.78      0.79      0.78    670771\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   discarded       0.79      0.84      0.81    344938\n",
      "    question       0.81      0.80      0.80    307456\n",
      "     section       0.53      0.20      0.29     12384\n",
      "  subsection       0.47      0.07      0.11      9714\n",
      "\n",
      "    accuracy                           0.80    674492\n",
      "   macro avg       0.65      0.47      0.51    674492\n",
      "weighted avg       0.79      0.80      0.79    674492\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   discarded       0.78      0.84      0.81    338042\n",
      "    question       0.83      0.80      0.81    318462\n",
      "     section       0.56      0.20      0.29     13161\n",
      "  subsection       0.45      0.06      0.10      9478\n",
      "\n",
      "    accuracy                           0.80    679143\n",
      "   macro avg       0.66      0.47      0.50    679143\n",
      "weighted avg       0.79      0.80      0.79    679143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for res in reports1:\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8bbfb81c-4ebe-46e6-9c55-740243dcd5f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run 1\n",
      "[LightGBM] [Info] Number of positive: 1135215, number of negative: 1303714\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.224365 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8503\n",
      "[LightGBM] [Info] Number of data points in the train set: 2438929, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.465456 -> initscore=-0.138395\n",
      "[LightGBM] [Info] Start training from score -0.138395\n",
      "[LightGBM] [Info] Number of positive: 1221432, number of negative: 1217497\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.231217 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8503\n",
      "[LightGBM] [Info] Number of data points in the train set: 2438929, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500807 -> initscore=0.003227\n",
      "[LightGBM] [Info] Start training from score 0.003227\n",
      "[LightGBM] [Info] Number of positive: 2403765, number of negative: 2403765\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.485167 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8083\n",
      "[LightGBM] [Info] Number of data points in the train set: 4807530, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47118, number of negative: 2391811\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.236276 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8503\n",
      "[LightGBM] [Info] Number of data points in the train set: 2438929, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.019319 -> initscore=-3.927151\n",
      "[LightGBM] [Info] Start training from score -3.927151\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   discarded       0.81      0.74      0.77    343880\n",
      "    question       0.81      0.78      0.79    313673\n",
      "     section       0.65      0.02      0.03     12516\n",
      "  subsection       0.11      0.70      0.20     10212\n",
      "\n",
      "    accuracy                           0.75    680281\n",
      "   macro avg       0.60      0.56      0.45    680281\n",
      "weighted avg       0.80      0.75      0.76    680281\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   discarded       0.81      0.74      0.77    343880\n",
      "    question       0.81      0.78      0.79    313673\n",
      "     section       0.65      0.02      0.03     12516\n",
      "  subsection       0.11      0.70      0.20     10212\n",
      "\n",
      "    accuracy                           0.75    680281\n",
      "   macro avg       0.60      0.56      0.45    680281\n",
      "weighted avg       0.80      0.75      0.76    680281\n",
      "\n",
      "run 2\n",
      "[LightGBM] [Info] Number of positive: 1138595, number of negative: 1318124\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.225692 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8365\n",
      "[LightGBM] [Info] Number of data points in the train set: 2456719, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.463462 -> initscore=-0.146414\n",
      "[LightGBM] [Info] Start training from score -0.146414\n",
      "[LightGBM] [Info] Number of positive: 1235033, number of negative: 1221686\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.234118 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8365\n",
      "[LightGBM] [Info] Number of data points in the train set: 2456719, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502716 -> initscore=0.010866\n",
      "[LightGBM] [Info] Start training from score 0.010866\n",
      "[LightGBM] [Info] Number of positive: 2420517, number of negative: 2420517\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.458741 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7985\n",
      "[LightGBM] [Info] Number of data points in the train set: 4841034, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 46889, number of negative: 2409830\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.239396 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8365\n",
      "[LightGBM] [Info] Number of data points in the train set: 2456719, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.019086 -> initscore=-3.939528\n",
      "[LightGBM] [Info] Start training from score -3.939528\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   discarded       0.81      0.76      0.78    330279\n",
      "    question       0.83      0.79      0.81    310293\n",
      "     section       0.76      0.01      0.03     12745\n",
      "  subsection       0.11      0.68      0.18      9174\n",
      "\n",
      "    accuracy                           0.76    662491\n",
      "   macro avg       0.63      0.56      0.45    662491\n",
      "weighted avg       0.81      0.76      0.77    662491\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   discarded       0.81      0.76      0.78    330279\n",
      "    question       0.83      0.79      0.81    310293\n",
      "     section       0.76      0.01      0.03     12745\n",
      "  subsection       0.11      0.68      0.18      9174\n",
      "\n",
      "    accuracy                           0.76    662491\n",
      "   macro avg       0.63      0.56      0.45    662491\n",
      "weighted avg       0.81      0.76      0.77    662491\n",
      "\n",
      "run 3\n",
      "[LightGBM] [Info] Number of positive: 1133648, number of negative: 1314791\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.232480 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8482\n",
      "[LightGBM] [Info] Number of data points in the train set: 2448439, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.463008 -> initscore=-0.148237\n",
      "[LightGBM] [Info] Start training from score -0.148237\n",
      "[LightGBM] [Info] Number of positive: 1232274, number of negative: 1216165\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.230745 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8482\n",
      "[LightGBM] [Info] Number of data points in the train set: 2448439, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503290 -> initscore=0.013159\n",
      "[LightGBM] [Info] Start training from score 0.013159\n",
      "[LightGBM] [Info] Number of positive: 2412919, number of negative: 2412919\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.480630 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8015\n",
      "[LightGBM] [Info] Number of data points in the train set: 4825838, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 46997, number of negative: 2401442\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.238906 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8482\n",
      "[LightGBM] [Info] Number of data points in the train set: 2448439, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.019195 -> initscore=-3.933741\n",
      "[LightGBM] [Info] Start training from score -3.933741\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   discarded       0.80      0.75      0.77    333038\n",
      "    question       0.82      0.78      0.80    315240\n",
      "     section       0.60      0.02      0.03     12637\n",
      "  subsection       0.12      0.72      0.20      9856\n",
      "\n",
      "    accuracy                           0.75    670771\n",
      "   macro avg       0.58      0.56      0.45    670771\n",
      "weighted avg       0.80      0.75      0.76    670771\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   discarded       0.80      0.75      0.77    333038\n",
      "    question       0.82      0.78      0.80    315240\n",
      "     section       0.60      0.02      0.03     12637\n",
      "  subsection       0.12      0.72      0.20      9856\n",
      "\n",
      "    accuracy                           0.75    670771\n",
      "   macro avg       0.58      0.56      0.45    670771\n",
      "weighted avg       0.80      0.75      0.76    670771\n",
      "\n",
      "run 4\n",
      "[LightGBM] [Info] Number of positive: 1141432, number of negative: 1303286\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.233292 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8427\n",
      "[LightGBM] [Info] Number of data points in the train set: 2444718, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.466897 -> initscore=-0.132605\n",
      "[LightGBM] [Info] Start training from score -0.132605\n",
      "[LightGBM] [Info] Number of positive: 1220374, number of negative: 1224344\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.227537 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8427\n",
      "[LightGBM] [Info] Number of data points in the train set: 2444718, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499188 -> initscore=-0.003248\n",
      "[LightGBM] [Info] Start training from score -0.003248\n",
      "[LightGBM] [Info] Number of positive: 2409056, number of negative: 2409056\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.452874 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8069\n",
      "[LightGBM] [Info] Number of data points in the train set: 4818112, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 47250, number of negative: 2397468\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.238638 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8427\n",
      "[LightGBM] [Info] Number of data points in the train set: 2444718, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.019327 -> initscore=-3.926716\n",
      "[LightGBM] [Info] Start training from score -3.926716\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   discarded       0.82      0.76      0.79    344938\n",
      "    question       0.82      0.78      0.80    307456\n",
      "     section       0.62      0.01      0.03     12384\n",
      "  subsection       0.11      0.70      0.19      9714\n",
      "\n",
      "    accuracy                           0.75    674492\n",
      "   macro avg       0.59      0.56      0.45    674492\n",
      "weighted avg       0.80      0.75      0.77    674492\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   discarded       0.82      0.76      0.79    344938\n",
      "    question       0.82      0.78      0.80    307456\n",
      "     section       0.62      0.01      0.03     12384\n",
      "  subsection       0.11      0.70      0.19      9714\n",
      "\n",
      "    accuracy                           0.75    674492\n",
      "   macro avg       0.59      0.56      0.45    674492\n",
      "weighted avg       0.80      0.75      0.77    674492\n",
      "\n",
      "run 5\n",
      "[LightGBM] [Info] Number of positive: 1130426, number of negative: 1309641\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.229926 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8463\n",
      "[LightGBM] [Info] Number of data points in the train set: 2440067, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.463277 -> initscore=-0.147159\n",
      "[LightGBM] [Info] Start training from score -0.147159\n",
      "[LightGBM] [Info] Number of positive: 1227270, number of negative: 1212797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.230091 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8463\n",
      "[LightGBM] [Info] Number of data points in the train set: 2440067, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502966 -> initscore=0.011863\n",
      "[LightGBM] [Info] Start training from score 0.011863\n",
      "[LightGBM] [Info] Number of positive: 2404169, number of negative: 2404169\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.496263 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8152\n",
      "[LightGBM] [Info] Number of data points in the train set: 4808338, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 46473, number of negative: 2393594\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.232303 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8463\n",
      "[LightGBM] [Info] Number of data points in the train set: 2440067, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.019046 -> initscore=-3.941680\n",
      "[LightGBM] [Info] Start training from score -3.941680\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   discarded       0.81      0.76      0.79    338042\n",
      "    question       0.83      0.78      0.81    318462\n",
      "     section       0.62      0.01      0.03     13161\n",
      "  subsection       0.11      0.69      0.18      9478\n",
      "\n",
      "    accuracy                           0.76    679143\n",
      "   macro avg       0.59      0.56      0.45    679143\n",
      "weighted avg       0.81      0.76      0.77    679143\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   discarded       0.81      0.76      0.79    338042\n",
      "    question       0.83      0.78      0.81    318462\n",
      "     section       0.62      0.01      0.03     13161\n",
      "  subsection       0.11      0.69      0.18      9478\n",
      "\n",
      "    accuracy                           0.76    679143\n",
      "   macro avg       0.59      0.56      0.45    679143\n",
      "weighted avg       0.81      0.76      0.77    679143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reports2 = []\n",
    "\n",
    "for r in r_state:\n",
    "    print(f'run {r}')\n",
    "    train_df = train_df_dict[r]\n",
    "    test_df = test_df_dict[r]\n",
    "    z = multi_pred_stats([train_test('question')[0],train_test('discarded')[0],train_test('subsection','over')[0],train_test('section')[0]])\n",
    "    reports2.append(z)\n",
    "    print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f783d707-0fae-4c13-8799-b530c9d74154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   discarded       0.81      0.74      0.77    343880\n",
      "    question       0.81      0.78      0.79    313673\n",
      "     section       0.65      0.02      0.03     12516\n",
      "  subsection       0.11      0.70      0.20     10212\n",
      "\n",
      "    accuracy                           0.75    680281\n",
      "   macro avg       0.60      0.56      0.45    680281\n",
      "weighted avg       0.80      0.75      0.76    680281\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   discarded       0.81      0.76      0.78    330279\n",
      "    question       0.83      0.79      0.81    310293\n",
      "     section       0.76      0.01      0.03     12745\n",
      "  subsection       0.11      0.68      0.18      9174\n",
      "\n",
      "    accuracy                           0.76    662491\n",
      "   macro avg       0.63      0.56      0.45    662491\n",
      "weighted avg       0.81      0.76      0.77    662491\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   discarded       0.80      0.75      0.77    333038\n",
      "    question       0.82      0.78      0.80    315240\n",
      "     section       0.60      0.02      0.03     12637\n",
      "  subsection       0.12      0.72      0.20      9856\n",
      "\n",
      "    accuracy                           0.75    670771\n",
      "   macro avg       0.58      0.56      0.45    670771\n",
      "weighted avg       0.80      0.75      0.76    670771\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   discarded       0.82      0.76      0.79    344938\n",
      "    question       0.82      0.78      0.80    307456\n",
      "     section       0.62      0.01      0.03     12384\n",
      "  subsection       0.11      0.70      0.19      9714\n",
      "\n",
      "    accuracy                           0.75    674492\n",
      "   macro avg       0.59      0.56      0.45    674492\n",
      "weighted avg       0.80      0.75      0.77    674492\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   discarded       0.81      0.76      0.79    338042\n",
      "    question       0.83      0.78      0.81    318462\n",
      "     section       0.62      0.01      0.03     13161\n",
      "  subsection       0.11      0.69      0.18      9478\n",
      "\n",
      "    accuracy                           0.76    679143\n",
      "   macro avg       0.59      0.56      0.45    679143\n",
      "weighted avg       0.81      0.76      0.77    679143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for res in reports2:\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "12e60b1a-6519-4c2e-bc73-e368440750da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run 1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8341182143924175, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8341182143924175\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.1233911067827616, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.1233911067827616\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.8182496720710062, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.8182496720710062\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9702107536403743, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9702107536403743\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8341182143924175, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8341182143924175\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.1233911067827616, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.1233911067827616\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.8182496720710062, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.8182496720710062\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9702107536403743, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9702107536403743\n",
      "[LightGBM] [Info] Number of positive: 1135215, number of negative: 1303714\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.225908 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8503\n",
      "[LightGBM] [Info] Number of data points in the train set: 2438929, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.465456 -> initscore=-0.138395\n",
      "[LightGBM] [Info] Start training from score -0.138395\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8341182143924175, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8341182143924175\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.1233911067827616, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.1233911067827616\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.8182496720710062, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.8182496720710062\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9702107536403743, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9702107536403743\n",
      "[LightGBM] [Warning] feature_fraction is set=0.45028179026459975, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.45028179026459975\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.1971042837442278, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.1971042837442278\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.297245941070125, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.297245941070125\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6243816730492119, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6243816730492119\n",
      "[LightGBM] [Warning] feature_fraction is set=0.45028179026459975, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.45028179026459975\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.1971042837442278, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.1971042837442278\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.297245941070125, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.297245941070125\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6243816730492119, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6243816730492119\n",
      "[LightGBM] [Info] Number of positive: 1217497, number of negative: 1217497\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.250570 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8491\n",
      "[LightGBM] [Info] Number of data points in the train set: 2434994, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] feature_fraction is set=0.45028179026459975, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.45028179026459975\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.1971042837442278, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.1971042837442278\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.297245941070125, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.297245941070125\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6243816730492119, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6243816730492119\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8620892013062822, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8620892013062822\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.979058792848589, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.979058792848589\n",
      "[LightGBM] [Warning] lambda_l2 is set=6.228517664360759, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.228517664360759\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.09154053332582732, subsample=1.0 will be ignored. Current value: bagging_fraction=0.09154053332582732\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8620892013062822, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8620892013062822\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.979058792848589, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.979058792848589\n",
      "[LightGBM] [Warning] lambda_l2 is set=6.228517664360759, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.228517664360759\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.09154053332582732, subsample=1.0 will be ignored. Current value: bagging_fraction=0.09154053332582732\n",
      "[LightGBM] [Info] Number of positive: 2403765, number of negative: 2403765\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.470644 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8083\n",
      "[LightGBM] [Info] Number of data points in the train set: 4807530, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8620892013062822, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8620892013062822\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.979058792848589, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.979058792848589\n",
      "[LightGBM] [Warning] lambda_l2 is set=6.228517664360759, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.228517664360759\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.09154053332582732, subsample=1.0 will be ignored. Current value: bagging_fraction=0.09154053332582732\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] lambda_l1 is set=3.574560893629339, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.574560893629339\n",
      "[LightGBM] [Warning] lambda_l2 is set=9.818056164657532, reg_lambda=0.0 will be ignored. Current value: lambda_l2=9.818056164657532\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.3633904999185218, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3633904999185218\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] lambda_l1 is set=3.574560893629339, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.574560893629339\n",
      "[LightGBM] [Warning] lambda_l2 is set=9.818056164657532, reg_lambda=0.0 will be ignored. Current value: lambda_l2=9.818056164657532\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.3633904999185218, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3633904999185218\n",
      "[LightGBM] [Info] Number of positive: 2391811, number of negative: 2391811\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.460017 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8149\n",
      "[LightGBM] [Info] Number of data points in the train set: 4783622, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] lambda_l1 is set=3.574560893629339, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.574560893629339\n",
      "[LightGBM] [Warning] lambda_l2 is set=9.818056164657532, reg_lambda=0.0 will be ignored. Current value: lambda_l2=9.818056164657532\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.3633904999185218, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3633904999185218\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   discarded       0.82      0.75      0.78    343880\n",
      "    question       0.82      0.79      0.81    313673\n",
      "     section       0.27      0.65      0.39     12516\n",
      "  subsection       0.13      0.42      0.20     10212\n",
      "\n",
      "    accuracy                           0.76    680281\n",
      "   macro avg       0.51      0.65      0.54    680281\n",
      "weighted avg       0.80      0.76      0.78    680281\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   discarded       0.82      0.75      0.78    343880\n",
      "    question       0.82      0.79      0.81    313673\n",
      "     section       0.27      0.65      0.39     12516\n",
      "  subsection       0.13      0.42      0.20     10212\n",
      "\n",
      "    accuracy                           0.76    680281\n",
      "   macro avg       0.51      0.65      0.54    680281\n",
      "weighted avg       0.80      0.76      0.78    680281\n",
      "\n",
      "run 2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8341182143924175, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8341182143924175\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.1233911067827616, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.1233911067827616\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.8182496720710062, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.8182496720710062\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9702107536403743, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9702107536403743\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8341182143924175, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8341182143924175\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.1233911067827616, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.1233911067827616\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.8182496720710062, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.8182496720710062\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9702107536403743, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9702107536403743\n",
      "[LightGBM] [Info] Number of positive: 1138595, number of negative: 1318124\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.223309 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8365\n",
      "[LightGBM] [Info] Number of data points in the train set: 2456719, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.463462 -> initscore=-0.146414\n",
      "[LightGBM] [Info] Start training from score -0.146414\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8341182143924175, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8341182143924175\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.1233911067827616, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.1233911067827616\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.8182496720710062, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.8182496720710062\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9702107536403743, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9702107536403743\n",
      "[LightGBM] [Warning] feature_fraction is set=0.45028179026459975, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.45028179026459975\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.1971042837442278, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.1971042837442278\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.297245941070125, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.297245941070125\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6243816730492119, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6243816730492119\n",
      "[LightGBM] [Warning] feature_fraction is set=0.45028179026459975, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.45028179026459975\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.1971042837442278, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.1971042837442278\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.297245941070125, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.297245941070125\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6243816730492119, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6243816730492119\n",
      "[LightGBM] [Info] Number of positive: 1221686, number of negative: 1221686\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.232724 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8458\n",
      "[LightGBM] [Info] Number of data points in the train set: 2443372, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] feature_fraction is set=0.45028179026459975, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.45028179026459975\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.1971042837442278, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.1971042837442278\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.297245941070125, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.297245941070125\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6243816730492119, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6243816730492119\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8620892013062822, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8620892013062822\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.979058792848589, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.979058792848589\n",
      "[LightGBM] [Warning] lambda_l2 is set=6.228517664360759, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.228517664360759\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.09154053332582732, subsample=1.0 will be ignored. Current value: bagging_fraction=0.09154053332582732\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8620892013062822, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8620892013062822\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.979058792848589, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.979058792848589\n",
      "[LightGBM] [Warning] lambda_l2 is set=6.228517664360759, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.228517664360759\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.09154053332582732, subsample=1.0 will be ignored. Current value: bagging_fraction=0.09154053332582732\n",
      "[LightGBM] [Info] Number of positive: 2420517, number of negative: 2420517\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.457611 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7985\n",
      "[LightGBM] [Info] Number of data points in the train set: 4841034, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8620892013062822, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8620892013062822\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.979058792848589, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.979058792848589\n",
      "[LightGBM] [Warning] lambda_l2 is set=6.228517664360759, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.228517664360759\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.09154053332582732, subsample=1.0 will be ignored. Current value: bagging_fraction=0.09154053332582732\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] lambda_l1 is set=3.574560893629339, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.574560893629339\n",
      "[LightGBM] [Warning] lambda_l2 is set=9.818056164657532, reg_lambda=0.0 will be ignored. Current value: lambda_l2=9.818056164657532\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.3633904999185218, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3633904999185218\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] lambda_l1 is set=3.574560893629339, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.574560893629339\n",
      "[LightGBM] [Warning] lambda_l2 is set=9.818056164657532, reg_lambda=0.0 will be ignored. Current value: lambda_l2=9.818056164657532\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.3633904999185218, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3633904999185218\n",
      "[LightGBM] [Info] Number of positive: 2409830, number of negative: 2409830\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.477401 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8108\n",
      "[LightGBM] [Info] Number of data points in the train set: 4819660, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] lambda_l1 is set=3.574560893629339, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.574560893629339\n",
      "[LightGBM] [Warning] lambda_l2 is set=9.818056164657532, reg_lambda=0.0 will be ignored. Current value: lambda_l2=9.818056164657532\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.3633904999185218, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3633904999185218\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   discarded       0.82      0.76      0.79    330279\n",
      "    question       0.84      0.80      0.82    310293\n",
      "     section       0.29      0.64      0.40     12745\n",
      "  subsection       0.12      0.42      0.19      9174\n",
      "\n",
      "    accuracy                           0.77    662491\n",
      "   macro avg       0.52      0.66      0.55    662491\n",
      "weighted avg       0.81      0.77      0.79    662491\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   discarded       0.82      0.76      0.79    330279\n",
      "    question       0.84      0.80      0.82    310293\n",
      "     section       0.29      0.64      0.40     12745\n",
      "  subsection       0.12      0.42      0.19      9174\n",
      "\n",
      "    accuracy                           0.77    662491\n",
      "   macro avg       0.52      0.66      0.55    662491\n",
      "weighted avg       0.81      0.77      0.79    662491\n",
      "\n",
      "run 3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8341182143924175, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8341182143924175\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.1233911067827616, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.1233911067827616\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.8182496720710062, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.8182496720710062\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9702107536403743, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9702107536403743\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8341182143924175, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8341182143924175\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.1233911067827616, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.1233911067827616\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.8182496720710062, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.8182496720710062\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9702107536403743, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9702107536403743\n",
      "[LightGBM] [Info] Number of positive: 1133648, number of negative: 1314791\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.236043 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8482\n",
      "[LightGBM] [Info] Number of data points in the train set: 2448439, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.463008 -> initscore=-0.148237\n",
      "[LightGBM] [Info] Start training from score -0.148237\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8341182143924175, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8341182143924175\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.1233911067827616, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.1233911067827616\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.8182496720710062, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.8182496720710062\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9702107536403743, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9702107536403743\n",
      "[LightGBM] [Warning] feature_fraction is set=0.45028179026459975, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.45028179026459975\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.1971042837442278, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.1971042837442278\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.297245941070125, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.297245941070125\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6243816730492119, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6243816730492119\n",
      "[LightGBM] [Warning] feature_fraction is set=0.45028179026459975, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.45028179026459975\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.1971042837442278, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.1971042837442278\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.297245941070125, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.297245941070125\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6243816730492119, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6243816730492119\n",
      "[LightGBM] [Info] Number of positive: 1216165, number of negative: 1216165\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.221397 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8467\n",
      "[LightGBM] [Info] Number of data points in the train set: 2432330, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] feature_fraction is set=0.45028179026459975, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.45028179026459975\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.1971042837442278, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.1971042837442278\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.297245941070125, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.297245941070125\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6243816730492119, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6243816730492119\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8620892013062822, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8620892013062822\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.979058792848589, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.979058792848589\n",
      "[LightGBM] [Warning] lambda_l2 is set=6.228517664360759, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.228517664360759\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.09154053332582732, subsample=1.0 will be ignored. Current value: bagging_fraction=0.09154053332582732\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8620892013062822, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8620892013062822\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.979058792848589, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.979058792848589\n",
      "[LightGBM] [Warning] lambda_l2 is set=6.228517664360759, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.228517664360759\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.09154053332582732, subsample=1.0 will be ignored. Current value: bagging_fraction=0.09154053332582732\n",
      "[LightGBM] [Info] Number of positive: 2412919, number of negative: 2412919\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.457931 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8015\n",
      "[LightGBM] [Info] Number of data points in the train set: 4825838, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8620892013062822, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8620892013062822\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.979058792848589, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.979058792848589\n",
      "[LightGBM] [Warning] lambda_l2 is set=6.228517664360759, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.228517664360759\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.09154053332582732, subsample=1.0 will be ignored. Current value: bagging_fraction=0.09154053332582732\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] lambda_l1 is set=3.574560893629339, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.574560893629339\n",
      "[LightGBM] [Warning] lambda_l2 is set=9.818056164657532, reg_lambda=0.0 will be ignored. Current value: lambda_l2=9.818056164657532\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.3633904999185218, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3633904999185218\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] lambda_l1 is set=3.574560893629339, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.574560893629339\n",
      "[LightGBM] [Warning] lambda_l2 is set=9.818056164657532, reg_lambda=0.0 will be ignored. Current value: lambda_l2=9.818056164657532\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.3633904999185218, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3633904999185218\n",
      "[LightGBM] [Info] Number of positive: 2401442, number of negative: 2401442\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.488385 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8277\n",
      "[LightGBM] [Info] Number of data points in the train set: 4802884, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] lambda_l1 is set=3.574560893629339, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.574560893629339\n",
      "[LightGBM] [Warning] lambda_l2 is set=9.818056164657532, reg_lambda=0.0 will be ignored. Current value: lambda_l2=9.818056164657532\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.3633904999185218, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3633904999185218\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   discarded       0.81      0.75      0.78    333038\n",
      "    question       0.83      0.79      0.81    315240\n",
      "     section       0.28      0.65      0.39     12637\n",
      "  subsection       0.13      0.43      0.20      9856\n",
      "\n",
      "    accuracy                           0.76    670771\n",
      "   macro avg       0.51      0.66      0.55    670771\n",
      "weighted avg       0.80      0.76      0.78    670771\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   discarded       0.81      0.75      0.78    333038\n",
      "    question       0.83      0.79      0.81    315240\n",
      "     section       0.28      0.65      0.39     12637\n",
      "  subsection       0.13      0.43      0.20      9856\n",
      "\n",
      "    accuracy                           0.76    670771\n",
      "   macro avg       0.51      0.66      0.55    670771\n",
      "weighted avg       0.80      0.76      0.78    670771\n",
      "\n",
      "run 4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8341182143924175, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8341182143924175\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.1233911067827616, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.1233911067827616\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.8182496720710062, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.8182496720710062\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9702107536403743, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9702107536403743\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8341182143924175, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8341182143924175\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.1233911067827616, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.1233911067827616\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.8182496720710062, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.8182496720710062\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9702107536403743, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9702107536403743\n",
      "[LightGBM] [Info] Number of positive: 1141432, number of negative: 1303286\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.234089 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8427\n",
      "[LightGBM] [Info] Number of data points in the train set: 2444718, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.466897 -> initscore=-0.132605\n",
      "[LightGBM] [Info] Start training from score -0.132605\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8341182143924175, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8341182143924175\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.1233911067827616, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.1233911067827616\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.8182496720710062, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.8182496720710062\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9702107536403743, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9702107536403743\n",
      "[LightGBM] [Warning] feature_fraction is set=0.45028179026459975, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.45028179026459975\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.1971042837442278, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.1971042837442278\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.297245941070125, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.297245941070125\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6243816730492119, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6243816730492119\n",
      "[LightGBM] [Warning] feature_fraction is set=0.45028179026459975, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.45028179026459975\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.1971042837442278, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.1971042837442278\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.297245941070125, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.297245941070125\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6243816730492119, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6243816730492119\n",
      "[LightGBM] [Info] Number of positive: 1220374, number of negative: 1220374\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.240862 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8351\n",
      "[LightGBM] [Info] Number of data points in the train set: 2440748, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] feature_fraction is set=0.45028179026459975, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.45028179026459975\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.1971042837442278, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.1971042837442278\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.297245941070125, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.297245941070125\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6243816730492119, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6243816730492119\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8620892013062822, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8620892013062822\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.979058792848589, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.979058792848589\n",
      "[LightGBM] [Warning] lambda_l2 is set=6.228517664360759, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.228517664360759\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.09154053332582732, subsample=1.0 will be ignored. Current value: bagging_fraction=0.09154053332582732\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8620892013062822, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8620892013062822\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.979058792848589, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.979058792848589\n",
      "[LightGBM] [Warning] lambda_l2 is set=6.228517664360759, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.228517664360759\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.09154053332582732, subsample=1.0 will be ignored. Current value: bagging_fraction=0.09154053332582732\n",
      "[LightGBM] [Info] Number of positive: 2409056, number of negative: 2409056\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.496889 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8069\n",
      "[LightGBM] [Info] Number of data points in the train set: 4818112, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8620892013062822, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8620892013062822\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.979058792848589, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.979058792848589\n",
      "[LightGBM] [Warning] lambda_l2 is set=6.228517664360759, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.228517664360759\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.09154053332582732, subsample=1.0 will be ignored. Current value: bagging_fraction=0.09154053332582732\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] lambda_l1 is set=3.574560893629339, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.574560893629339\n",
      "[LightGBM] [Warning] lambda_l2 is set=9.818056164657532, reg_lambda=0.0 will be ignored. Current value: lambda_l2=9.818056164657532\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.3633904999185218, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3633904999185218\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] lambda_l1 is set=3.574560893629339, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.574560893629339\n",
      "[LightGBM] [Warning] lambda_l2 is set=9.818056164657532, reg_lambda=0.0 will be ignored. Current value: lambda_l2=9.818056164657532\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.3633904999185218, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3633904999185218\n",
      "[LightGBM] [Info] Number of positive: 2397468, number of negative: 2397468\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.469614 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8101\n",
      "[LightGBM] [Info] Number of data points in the train set: 4794936, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] lambda_l1 is set=3.574560893629339, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.574560893629339\n",
      "[LightGBM] [Warning] lambda_l2 is set=9.818056164657532, reg_lambda=0.0 will be ignored. Current value: lambda_l2=9.818056164657532\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.3633904999185218, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3633904999185218\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   discarded       0.83      0.76      0.79    344938\n",
      "    question       0.83      0.79      0.81    307456\n",
      "     section       0.28      0.65      0.39     12384\n",
      "  subsection       0.13      0.42      0.19      9714\n",
      "\n",
      "    accuracy                           0.77    674492\n",
      "   macro avg       0.51      0.66      0.55    674492\n",
      "weighted avg       0.81      0.77      0.78    674492\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   discarded       0.83      0.76      0.79    344938\n",
      "    question       0.83      0.79      0.81    307456\n",
      "     section       0.28      0.65      0.39     12384\n",
      "  subsection       0.13      0.42      0.19      9714\n",
      "\n",
      "    accuracy                           0.77    674492\n",
      "   macro avg       0.51      0.66      0.55    674492\n",
      "weighted avg       0.81      0.77      0.78    674492\n",
      "\n",
      "run 5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8341182143924175, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8341182143924175\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.1233911067827616, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.1233911067827616\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.8182496720710062, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.8182496720710062\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9702107536403743, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9702107536403743\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8341182143924175, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8341182143924175\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.1233911067827616, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.1233911067827616\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.8182496720710062, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.8182496720710062\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9702107536403743, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9702107536403743\n",
      "[LightGBM] [Info] Number of positive: 1130426, number of negative: 1309641\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.241915 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8463\n",
      "[LightGBM] [Info] Number of data points in the train set: 2440067, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.463277 -> initscore=-0.147159\n",
      "[LightGBM] [Info] Start training from score -0.147159\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8341182143924175, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8341182143924175\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.1233911067827616, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.1233911067827616\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.8182496720710062, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.8182496720710062\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9702107536403743, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9702107536403743\n",
      "[LightGBM] [Warning] feature_fraction is set=0.45028179026459975, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.45028179026459975\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.1971042837442278, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.1971042837442278\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.297245941070125, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.297245941070125\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6243816730492119, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6243816730492119\n",
      "[LightGBM] [Warning] feature_fraction is set=0.45028179026459975, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.45028179026459975\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.1971042837442278, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.1971042837442278\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.297245941070125, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.297245941070125\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6243816730492119, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6243816730492119\n",
      "[LightGBM] [Info] Number of positive: 1212797, number of negative: 1212797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.234325 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8457\n",
      "[LightGBM] [Info] Number of data points in the train set: 2425594, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] feature_fraction is set=0.45028179026459975, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.45028179026459975\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.1971042837442278, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.1971042837442278\n",
      "[LightGBM] [Warning] lambda_l2 is set=4.297245941070125, reg_lambda=0.0 will be ignored. Current value: lambda_l2=4.297245941070125\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6243816730492119, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6243816730492119\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8620892013062822, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8620892013062822\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.979058792848589, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.979058792848589\n",
      "[LightGBM] [Warning] lambda_l2 is set=6.228517664360759, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.228517664360759\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.09154053332582732, subsample=1.0 will be ignored. Current value: bagging_fraction=0.09154053332582732\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8620892013062822, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8620892013062822\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.979058792848589, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.979058792848589\n",
      "[LightGBM] [Warning] lambda_l2 is set=6.228517664360759, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.228517664360759\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.09154053332582732, subsample=1.0 will be ignored. Current value: bagging_fraction=0.09154053332582732\n",
      "[LightGBM] [Info] Number of positive: 2404169, number of negative: 2404169\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.472153 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8152\n",
      "[LightGBM] [Info] Number of data points in the train set: 4808338, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8620892013062822, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8620892013062822\n",
      "[LightGBM] [Warning] lambda_l1 is set=2.979058792848589, reg_alpha=0.0 will be ignored. Current value: lambda_l1=2.979058792848589\n",
      "[LightGBM] [Warning] lambda_l2 is set=6.228517664360759, reg_lambda=0.0 will be ignored. Current value: lambda_l2=6.228517664360759\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.09154053332582732, subsample=1.0 will be ignored. Current value: bagging_fraction=0.09154053332582732\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] lambda_l1 is set=3.574560893629339, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.574560893629339\n",
      "[LightGBM] [Warning] lambda_l2 is set=9.818056164657532, reg_lambda=0.0 will be ignored. Current value: lambda_l2=9.818056164657532\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.3633904999185218, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3633904999185218\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] lambda_l1 is set=3.574560893629339, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.574560893629339\n",
      "[LightGBM] [Warning] lambda_l2 is set=9.818056164657532, reg_lambda=0.0 will be ignored. Current value: lambda_l2=9.818056164657532\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.3633904999185218, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3633904999185218\n",
      "[LightGBM] [Info] Number of positive: 2393594, number of negative: 2393594\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.455923 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8104\n",
      "[LightGBM] [Info] Number of data points in the train set: 4787188, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] lambda_l1 is set=3.574560893629339, reg_alpha=0.0 will be ignored. Current value: lambda_l1=3.574560893629339\n",
      "[LightGBM] [Warning] lambda_l2 is set=9.818056164657532, reg_lambda=0.0 will be ignored. Current value: lambda_l2=9.818056164657532\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.3633904999185218, subsample=1.0 will be ignored. Current value: bagging_fraction=0.3633904999185218\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   discarded       0.82      0.77      0.79    338042\n",
      "    question       0.84      0.80      0.82    318462\n",
      "     section       0.29      0.64      0.40     13161\n",
      "  subsection       0.12      0.41      0.18      9478\n",
      "\n",
      "    accuracy                           0.77    679143\n",
      "   macro avg       0.52      0.65      0.55    679143\n",
      "weighted avg       0.81      0.77      0.79    679143\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   discarded       0.82      0.77      0.79    338042\n",
      "    question       0.84      0.80      0.82    318462\n",
      "     section       0.29      0.64      0.40     13161\n",
      "  subsection       0.12      0.41      0.18      9478\n",
      "\n",
      "    accuracy                           0.77    679143\n",
      "   macro avg       0.52      0.65      0.55    679143\n",
      "weighted avg       0.81      0.77      0.79    679143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reports3 = []\n",
    "\n",
    "for r in r_state:\n",
    "    print(f'run {r}')\n",
    "    train_df = train_df_dict[r]\n",
    "    test_df = test_df_dict[r]\n",
    "    z = multi_pred_stats([train_test('question','none',params_q)[0],train_test('discarded','under',params_d)[0],train_test('subsection','over',params_ss)[0],train_test('section','over',params_s)[0]])\n",
    "    reports3.append(z)\n",
    "    print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1436d5f6-97d3-4799-a97c-5569bf144053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   discarded       0.82      0.75      0.78    343880\n",
      "    question       0.82      0.79      0.81    313673\n",
      "     section       0.27      0.65      0.39     12516\n",
      "  subsection       0.13      0.42      0.20     10212\n",
      "\n",
      "    accuracy                           0.76    680281\n",
      "   macro avg       0.51      0.65      0.54    680281\n",
      "weighted avg       0.80      0.76      0.78    680281\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   discarded       0.82      0.76      0.79    330279\n",
      "    question       0.84      0.80      0.82    310293\n",
      "     section       0.29      0.64      0.40     12745\n",
      "  subsection       0.12      0.42      0.19      9174\n",
      "\n",
      "    accuracy                           0.77    662491\n",
      "   macro avg       0.52      0.66      0.55    662491\n",
      "weighted avg       0.81      0.77      0.79    662491\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   discarded       0.81      0.75      0.78    333038\n",
      "    question       0.83      0.79      0.81    315240\n",
      "     section       0.28      0.65      0.39     12637\n",
      "  subsection       0.13      0.43      0.20      9856\n",
      "\n",
      "    accuracy                           0.76    670771\n",
      "   macro avg       0.51      0.66      0.55    670771\n",
      "weighted avg       0.80      0.76      0.78    670771\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   discarded       0.83      0.76      0.79    344938\n",
      "    question       0.83      0.79      0.81    307456\n",
      "     section       0.28      0.65      0.39     12384\n",
      "  subsection       0.13      0.42      0.19      9714\n",
      "\n",
      "    accuracy                           0.77    674492\n",
      "   macro avg       0.51      0.66      0.55    674492\n",
      "weighted avg       0.81      0.77      0.78    674492\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   discarded       0.82      0.77      0.79    338042\n",
      "    question       0.84      0.80      0.82    318462\n",
      "     section       0.29      0.64      0.40     13161\n",
      "  subsection       0.12      0.41      0.18      9478\n",
      "\n",
      "    accuracy                           0.77    679143\n",
      "   macro avg       0.52      0.65      0.55    679143\n",
      "weighted avg       0.81      0.77      0.79    679143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for res in reports3:\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0bc2c71e-c846-4735-a31d-d52720d2a665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run 1\n",
      "[LightGBM] [Info] Number of positive: 1135215, number of negative: 1303714\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.246543 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8503\n",
      "[LightGBM] [Info] Number of data points in the train set: 2438929, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.465456 -> initscore=-0.138395\n",
      "[LightGBM] [Info] Start training from score -0.138395\n",
      "[LightGBM] [Info] Number of positive: 1217497, number of negative: 1217497\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.371928 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8491\n",
      "[LightGBM] [Info] Number of data points in the train set: 2434994, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 2403765, number of negative: 2403765\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.498681 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8083\n",
      "[LightGBM] [Info] Number of data points in the train set: 4807530, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 2391811, number of negative: 2391811\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.495086 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8149\n",
      "[LightGBM] [Info] Number of data points in the train set: 4783622, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   discarded       0.81      0.73      0.77    343880\n",
      "    question       0.81      0.78      0.79    313673\n",
      "     section       0.26      0.67      0.37     12516\n",
      "  subsection       0.10      0.39      0.16     10212\n",
      "\n",
      "    accuracy                           0.74    680281\n",
      "   macro avg       0.50      0.64      0.52    680281\n",
      "weighted avg       0.79      0.74      0.76    680281\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   discarded       0.81      0.73      0.77    343880\n",
      "    question       0.81      0.78      0.79    313673\n",
      "     section       0.26      0.67      0.37     12516\n",
      "  subsection       0.10      0.39      0.16     10212\n",
      "\n",
      "    accuracy                           0.74    680281\n",
      "   macro avg       0.50      0.64      0.52    680281\n",
      "weighted avg       0.79      0.74      0.76    680281\n",
      "\n",
      "run 2\n",
      "[LightGBM] [Info] Number of positive: 1138595, number of negative: 1318124\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.285580 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8365\n",
      "[LightGBM] [Info] Number of data points in the train set: 2456719, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.463462 -> initscore=-0.146414\n",
      "[LightGBM] [Info] Start training from score -0.146414\n",
      "[LightGBM] [Info] Number of positive: 1221686, number of negative: 1221686\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.279844 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8458\n",
      "[LightGBM] [Info] Number of data points in the train set: 2443372, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 2420517, number of negative: 2420517\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.544142 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7985\n",
      "[LightGBM] [Info] Number of data points in the train set: 4841034, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 2409830, number of negative: 2409830\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.468478 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8108\n",
      "[LightGBM] [Info] Number of data points in the train set: 4819660, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   discarded       0.81      0.74      0.78    330279\n",
      "    question       0.83      0.78      0.81    310293\n",
      "     section       0.26      0.65      0.38     12745\n",
      "  subsection       0.10      0.39      0.16      9174\n",
      "\n",
      "    accuracy                           0.76    662491\n",
      "   macro avg       0.50      0.64      0.53    662491\n",
      "weighted avg       0.80      0.76      0.77    662491\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   discarded       0.81      0.74      0.78    330279\n",
      "    question       0.83      0.78      0.81    310293\n",
      "     section       0.26      0.65      0.38     12745\n",
      "  subsection       0.10      0.39      0.16      9174\n",
      "\n",
      "    accuracy                           0.76    662491\n",
      "   macro avg       0.50      0.64      0.53    662491\n",
      "weighted avg       0.80      0.76      0.77    662491\n",
      "\n",
      "run 3\n",
      "[LightGBM] [Info] Number of positive: 1133648, number of negative: 1314791\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.242660 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8482\n",
      "[LightGBM] [Info] Number of data points in the train set: 2448439, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.463008 -> initscore=-0.148237\n",
      "[LightGBM] [Info] Start training from score -0.148237\n",
      "[LightGBM] [Info] Number of positive: 1216165, number of negative: 1216165\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.239848 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8467\n",
      "[LightGBM] [Info] Number of data points in the train set: 2432330, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 2412919, number of negative: 2412919\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.543420 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8015\n",
      "[LightGBM] [Info] Number of data points in the train set: 4825838, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 2401442, number of negative: 2401442\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.554079 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8277\n",
      "[LightGBM] [Info] Number of data points in the train set: 4802884, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   discarded       0.81      0.73      0.77    333038\n",
      "    question       0.82      0.78      0.80    315240\n",
      "     section       0.27      0.68      0.39     12637\n",
      "  subsection       0.11      0.41      0.17      9856\n",
      "\n",
      "    accuracy                           0.75    670771\n",
      "   macro avg       0.50      0.65      0.53    670771\n",
      "weighted avg       0.79      0.75      0.77    670771\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   discarded       0.81      0.73      0.77    333038\n",
      "    question       0.82      0.78      0.80    315240\n",
      "     section       0.27      0.68      0.39     12637\n",
      "  subsection       0.11      0.41      0.17      9856\n",
      "\n",
      "    accuracy                           0.75    670771\n",
      "   macro avg       0.50      0.65      0.53    670771\n",
      "weighted avg       0.79      0.75      0.77    670771\n",
      "\n",
      "run 4\n",
      "[LightGBM] [Info] Number of positive: 1141432, number of negative: 1303286\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.221042 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8427\n",
      "[LightGBM] [Info] Number of data points in the train set: 2444718, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.466897 -> initscore=-0.132605\n",
      "[LightGBM] [Info] Start training from score -0.132605\n",
      "[LightGBM] [Info] Number of positive: 1220374, number of negative: 1220374\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.281012 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8351\n",
      "[LightGBM] [Info] Number of data points in the train set: 2440748, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 2409056, number of negative: 2409056\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.508349 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8069\n",
      "[LightGBM] [Info] Number of data points in the train set: 4818112, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 2397468, number of negative: 2397468\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.603253 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8101\n",
      "[LightGBM] [Info] Number of data points in the train set: 4794936, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   discarded       0.82      0.74      0.78    344938\n",
      "    question       0.82      0.78      0.80    307456\n",
      "     section       0.26      0.67      0.37     12384\n",
      "  subsection       0.10      0.40      0.16      9714\n",
      "\n",
      "    accuracy                           0.75    674492\n",
      "   macro avg       0.50      0.65      0.53    674492\n",
      "weighted avg       0.80      0.75      0.77    674492\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   discarded       0.82      0.74      0.78    344938\n",
      "    question       0.82      0.78      0.80    307456\n",
      "     section       0.26      0.67      0.37     12384\n",
      "  subsection       0.10      0.40      0.16      9714\n",
      "\n",
      "    accuracy                           0.75    674492\n",
      "   macro avg       0.50      0.65      0.53    674492\n",
      "weighted avg       0.80      0.75      0.77    674492\n",
      "\n",
      "run 5\n",
      "[LightGBM] [Info] Number of positive: 1130426, number of negative: 1309641\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.267305 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8463\n",
      "[LightGBM] [Info] Number of data points in the train set: 2440067, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.463277 -> initscore=-0.147159\n",
      "[LightGBM] [Info] Start training from score -0.147159\n",
      "[LightGBM] [Info] Number of positive: 1212797, number of negative: 1212797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.282820 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8457\n",
      "[LightGBM] [Info] Number of data points in the train set: 2425594, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 2404169, number of negative: 2404169\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.478975 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8152\n",
      "[LightGBM] [Info] Number of data points in the train set: 4808338, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 2393594, number of negative: 2393594\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.577373 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8104\n",
      "[LightGBM] [Info] Number of data points in the train set: 4787188, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   discarded       0.81      0.74      0.78    338042\n",
      "    question       0.83      0.79      0.81    318462\n",
      "     section       0.27      0.67      0.38     13161\n",
      "  subsection       0.10      0.38      0.15      9478\n",
      "\n",
      "    accuracy                           0.76    679143\n",
      "   macro avg       0.50      0.64      0.53    679143\n",
      "weighted avg       0.80      0.76      0.78    679143\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   discarded       0.81      0.74      0.78    338042\n",
      "    question       0.83      0.79      0.81    318462\n",
      "     section       0.27      0.67      0.38     13161\n",
      "  subsection       0.10      0.38      0.15      9478\n",
      "\n",
      "    accuracy                           0.76    679143\n",
      "   macro avg       0.50      0.64      0.53    679143\n",
      "weighted avg       0.80      0.76      0.78    679143\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   discarded       0.81      0.73      0.77    343880\n",
      "    question       0.81      0.78      0.79    313673\n",
      "     section       0.26      0.67      0.37     12516\n",
      "  subsection       0.10      0.39      0.16     10212\n",
      "\n",
      "    accuracy                           0.74    680281\n",
      "   macro avg       0.50      0.64      0.52    680281\n",
      "weighted avg       0.79      0.74      0.76    680281\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   discarded       0.81      0.74      0.78    330279\n",
      "    question       0.83      0.78      0.81    310293\n",
      "     section       0.26      0.65      0.38     12745\n",
      "  subsection       0.10      0.39      0.16      9174\n",
      "\n",
      "    accuracy                           0.76    662491\n",
      "   macro avg       0.50      0.64      0.53    662491\n",
      "weighted avg       0.80      0.76      0.77    662491\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   discarded       0.81      0.73      0.77    333038\n",
      "    question       0.82      0.78      0.80    315240\n",
      "     section       0.27      0.68      0.39     12637\n",
      "  subsection       0.11      0.41      0.17      9856\n",
      "\n",
      "    accuracy                           0.75    670771\n",
      "   macro avg       0.50      0.65      0.53    670771\n",
      "weighted avg       0.79      0.75      0.77    670771\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   discarded       0.82      0.74      0.78    344938\n",
      "    question       0.82      0.78      0.80    307456\n",
      "     section       0.26      0.67      0.37     12384\n",
      "  subsection       0.10      0.40      0.16      9714\n",
      "\n",
      "    accuracy                           0.75    674492\n",
      "   macro avg       0.50      0.65      0.53    674492\n",
      "weighted avg       0.80      0.75      0.77    674492\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   discarded       0.81      0.74      0.78    338042\n",
      "    question       0.83      0.79      0.81    318462\n",
      "     section       0.27      0.67      0.38     13161\n",
      "  subsection       0.10      0.38      0.15      9478\n",
      "\n",
      "    accuracy                           0.76    679143\n",
      "   macro avg       0.50      0.64      0.53    679143\n",
      "weighted avg       0.80      0.76      0.78    679143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reports7 = []\n",
    "\n",
    "for r in r_state:\n",
    "    print(f'run {r}')\n",
    "    train_df = train_df_dict[r]\n",
    "    test_df = test_df_dict[r]\n",
    "    z = multi_pred_stats([train_test('question','none')[0],train_test('discarded','under')[0],train_test('subsection','over')[0],train_test('section','over')[0]])\n",
    "    reports7.append(z)\n",
    "    print(z)\n",
    "for res in reports7:\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ac06a980-d1f3-4953-8ccd-a4bdb5b06dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run 1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.820049231809358, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.820049231809358\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.35398913607890153, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.35398913607890153\n",
      "[LightGBM] [Warning] lambda_l2 is set=2.413763938888623, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.413763938888623\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7939521214604877, subsample=0.3170686952817686 will be ignored. Current value: bagging_fraction=0.7939521214604877\n",
      "[LightGBM] [Warning] feature_fraction is set=0.820049231809358, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.820049231809358\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.35398913607890153, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.35398913607890153\n",
      "[LightGBM] [Warning] lambda_l2 is set=2.413763938888623, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.413763938888623\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7939521214604877, subsample=0.3170686952817686 will be ignored. Current value: bagging_fraction=0.7939521214604877\n",
      "[LightGBM] [Info] Number of positive: 1135215, number of negative: 1303714\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.228136 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8503\n",
      "[LightGBM] [Info] Number of data points in the train set: 2438929, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.465456 -> initscore=-0.138395\n",
      "[LightGBM] [Info] Start training from score -0.138395\n",
      "[LightGBM] [Warning] feature_fraction is set=0.820049231809358, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.820049231809358\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.35398913607890153, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.35398913607890153\n",
      "[LightGBM] [Warning] lambda_l2 is set=2.413763938888623, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.413763938888623\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7939521214604877, subsample=0.3170686952817686 will be ignored. Current value: bagging_fraction=0.7939521214604877\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] lambda_l1 is set=9.82085679893001, reg_alpha=0.0 will be ignored. Current value: lambda_l1=9.82085679893001\n",
      "[LightGBM] [Warning] lambda_l2 is set=7.043665039579396, reg_lambda=0.0 will be ignored. Current value: lambda_l2=7.043665039579396\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] lambda_l1 is set=9.82085679893001, reg_alpha=0.0 will be ignored. Current value: lambda_l1=9.82085679893001\n",
      "[LightGBM] [Warning] lambda_l2 is set=7.043665039579396, reg_lambda=0.0 will be ignored. Current value: lambda_l2=7.043665039579396\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Info] Number of positive: 1217497, number of negative: 1217497\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.228075 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8491\n",
      "[LightGBM] [Info] Number of data points in the train set: 2434994, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] lambda_l1 is set=9.82085679893001, reg_alpha=0.0 will be ignored. Current value: lambda_l1=9.82085679893001\n",
      "[LightGBM] [Warning] lambda_l2 is set=7.043665039579396, reg_lambda=0.0 will be ignored. Current value: lambda_l2=7.043665039579396\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8262199992945417, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8262199992945417\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4124684372749695, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4124684372749695\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.37512893628136457, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.37512893628136457\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5882322650485803, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5882322650485803\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8262199992945417, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8262199992945417\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4124684372749695, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4124684372749695\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.37512893628136457, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.37512893628136457\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5882322650485803, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5882322650485803\n",
      "[LightGBM] [Info] Number of positive: 35164, number of negative: 2403765\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.219510 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8503\n",
      "[LightGBM] [Info] Number of data points in the train set: 2438929, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014418 -> initscore=-4.224769\n",
      "[LightGBM] [Info] Start training from score -4.224769\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8262199992945417, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8262199992945417\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4124684372749695, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4124684372749695\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.37512893628136457, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.37512893628136457\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5882322650485803, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5882322650485803\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8633117565583335, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8633117565583335\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.55290923003859, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.55290923003859\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3492214895079347, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3492214895079347\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8505357588039386, subsample=0.9543648555046881 will be ignored. Current value: bagging_fraction=0.8505357588039386\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8633117565583335, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8633117565583335\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.55290923003859, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.55290923003859\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3492214895079347, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3492214895079347\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8505357588039386, subsample=0.9543648555046881 will be ignored. Current value: bagging_fraction=0.8505357588039386\n",
      "[LightGBM] [Info] Number of positive: 2391811, number of negative: 2391811\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.458304 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8149\n",
      "[LightGBM] [Info] Number of data points in the train set: 4783622, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8633117565583335, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8633117565583335\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.55290923003859, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.55290923003859\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3492214895079347, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3492214895079347\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8505357588039386, subsample=0.9543648555046881 will be ignored. Current value: bagging_fraction=0.8505357588039386\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   discarded       0.86      0.70      0.77    343880\n",
      "    question       0.76      0.87      0.82    313673\n",
      "     section       0.25      0.80      0.38     12516\n",
      "  subsection       0.70      0.01      0.01     10212\n",
      "\n",
      "    accuracy                           0.77    680281\n",
      "   macro avg       0.64      0.60      0.50    680281\n",
      "weighted avg       0.80      0.77      0.77    680281\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   discarded       0.86      0.70      0.77    343880\n",
      "    question       0.76      0.87      0.82    313673\n",
      "     section       0.25      0.80      0.38     12516\n",
      "  subsection       0.70      0.01      0.01     10212\n",
      "\n",
      "    accuracy                           0.77    680281\n",
      "   macro avg       0.64      0.60      0.50    680281\n",
      "weighted avg       0.80      0.77      0.77    680281\n",
      "\n",
      "run 2\n",
      "[LightGBM] [Warning] feature_fraction is set=0.820049231809358, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.820049231809358\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.35398913607890153, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.35398913607890153\n",
      "[LightGBM] [Warning] lambda_l2 is set=2.413763938888623, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.413763938888623\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7939521214604877, subsample=0.3170686952817686 will be ignored. Current value: bagging_fraction=0.7939521214604877\n",
      "[LightGBM] [Warning] feature_fraction is set=0.820049231809358, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.820049231809358\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.35398913607890153, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.35398913607890153\n",
      "[LightGBM] [Warning] lambda_l2 is set=2.413763938888623, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.413763938888623\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7939521214604877, subsample=0.3170686952817686 will be ignored. Current value: bagging_fraction=0.7939521214604877\n",
      "[LightGBM] [Info] Number of positive: 1138595, number of negative: 1318124\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.234817 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8365\n",
      "[LightGBM] [Info] Number of data points in the train set: 2456719, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.463462 -> initscore=-0.146414\n",
      "[LightGBM] [Info] Start training from score -0.146414\n",
      "[LightGBM] [Warning] feature_fraction is set=0.820049231809358, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.820049231809358\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.35398913607890153, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.35398913607890153\n",
      "[LightGBM] [Warning] lambda_l2 is set=2.413763938888623, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.413763938888623\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7939521214604877, subsample=0.3170686952817686 will be ignored. Current value: bagging_fraction=0.7939521214604877\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] lambda_l1 is set=9.82085679893001, reg_alpha=0.0 will be ignored. Current value: lambda_l1=9.82085679893001\n",
      "[LightGBM] [Warning] lambda_l2 is set=7.043665039579396, reg_lambda=0.0 will be ignored. Current value: lambda_l2=7.043665039579396\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] lambda_l1 is set=9.82085679893001, reg_alpha=0.0 will be ignored. Current value: lambda_l1=9.82085679893001\n",
      "[LightGBM] [Warning] lambda_l2 is set=7.043665039579396, reg_lambda=0.0 will be ignored. Current value: lambda_l2=7.043665039579396\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Info] Number of positive: 1221686, number of negative: 1221686\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.237641 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8458\n",
      "[LightGBM] [Info] Number of data points in the train set: 2443372, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] lambda_l1 is set=9.82085679893001, reg_alpha=0.0 will be ignored. Current value: lambda_l1=9.82085679893001\n",
      "[LightGBM] [Warning] lambda_l2 is set=7.043665039579396, reg_lambda=0.0 will be ignored. Current value: lambda_l2=7.043665039579396\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8262199992945417, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8262199992945417\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4124684372749695, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4124684372749695\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.37512893628136457, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.37512893628136457\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5882322650485803, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5882322650485803\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8262199992945417, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8262199992945417\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4124684372749695, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4124684372749695\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.37512893628136457, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.37512893628136457\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5882322650485803, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5882322650485803\n",
      "[LightGBM] [Info] Number of positive: 36202, number of negative: 2420517\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.200581 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8365\n",
      "[LightGBM] [Info] Number of data points in the train set: 2456719, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014736 -> initscore=-4.202622\n",
      "[LightGBM] [Info] Start training from score -4.202622\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8262199992945417, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8262199992945417\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4124684372749695, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4124684372749695\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.37512893628136457, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.37512893628136457\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5882322650485803, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5882322650485803\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8633117565583335, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8633117565583335\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.55290923003859, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.55290923003859\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3492214895079347, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3492214895079347\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8505357588039386, subsample=0.9543648555046881 will be ignored. Current value: bagging_fraction=0.8505357588039386\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8633117565583335, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8633117565583335\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.55290923003859, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.55290923003859\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3492214895079347, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3492214895079347\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8505357588039386, subsample=0.9543648555046881 will be ignored. Current value: bagging_fraction=0.8505357588039386\n",
      "[LightGBM] [Info] Number of positive: 2409830, number of negative: 2409830\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.460405 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8108\n",
      "[LightGBM] [Info] Number of data points in the train set: 4819660, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8633117565583335, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8633117565583335\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.55290923003859, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.55290923003859\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3492214895079347, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3492214895079347\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8505357588039386, subsample=0.9543648555046881 will be ignored. Current value: bagging_fraction=0.8505357588039386\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   discarded       0.86      0.72      0.78    330279\n",
      "    question       0.78      0.88      0.83    310293\n",
      "     section       0.26      0.79      0.39     12745\n",
      "  subsection       0.81      0.01      0.02      9174\n",
      "\n",
      "    accuracy                           0.78    662491\n",
      "   macro avg       0.68      0.60      0.50    662491\n",
      "weighted avg       0.81      0.78      0.79    662491\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   discarded       0.86      0.72      0.78    330279\n",
      "    question       0.78      0.88      0.83    310293\n",
      "     section       0.26      0.79      0.39     12745\n",
      "  subsection       0.81      0.01      0.02      9174\n",
      "\n",
      "    accuracy                           0.78    662491\n",
      "   macro avg       0.68      0.60      0.50    662491\n",
      "weighted avg       0.81      0.78      0.79    662491\n",
      "\n",
      "run 3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.820049231809358, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.820049231809358\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.35398913607890153, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.35398913607890153\n",
      "[LightGBM] [Warning] lambda_l2 is set=2.413763938888623, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.413763938888623\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7939521214604877, subsample=0.3170686952817686 will be ignored. Current value: bagging_fraction=0.7939521214604877\n",
      "[LightGBM] [Warning] feature_fraction is set=0.820049231809358, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.820049231809358\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.35398913607890153, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.35398913607890153\n",
      "[LightGBM] [Warning] lambda_l2 is set=2.413763938888623, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.413763938888623\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7939521214604877, subsample=0.3170686952817686 will be ignored. Current value: bagging_fraction=0.7939521214604877\n",
      "[LightGBM] [Info] Number of positive: 1133648, number of negative: 1314791\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.235886 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8482\n",
      "[LightGBM] [Info] Number of data points in the train set: 2448439, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.463008 -> initscore=-0.148237\n",
      "[LightGBM] [Info] Start training from score -0.148237\n",
      "[LightGBM] [Warning] feature_fraction is set=0.820049231809358, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.820049231809358\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.35398913607890153, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.35398913607890153\n",
      "[LightGBM] [Warning] lambda_l2 is set=2.413763938888623, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.413763938888623\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7939521214604877, subsample=0.3170686952817686 will be ignored. Current value: bagging_fraction=0.7939521214604877\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] lambda_l1 is set=9.82085679893001, reg_alpha=0.0 will be ignored. Current value: lambda_l1=9.82085679893001\n",
      "[LightGBM] [Warning] lambda_l2 is set=7.043665039579396, reg_lambda=0.0 will be ignored. Current value: lambda_l2=7.043665039579396\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] lambda_l1 is set=9.82085679893001, reg_alpha=0.0 will be ignored. Current value: lambda_l1=9.82085679893001\n",
      "[LightGBM] [Warning] lambda_l2 is set=7.043665039579396, reg_lambda=0.0 will be ignored. Current value: lambda_l2=7.043665039579396\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Info] Number of positive: 1216165, number of negative: 1216165\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.248806 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8467\n",
      "[LightGBM] [Info] Number of data points in the train set: 2432330, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] lambda_l1 is set=9.82085679893001, reg_alpha=0.0 will be ignored. Current value: lambda_l1=9.82085679893001\n",
      "[LightGBM] [Warning] lambda_l2 is set=7.043665039579396, reg_lambda=0.0 will be ignored. Current value: lambda_l2=7.043665039579396\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8262199992945417, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8262199992945417\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4124684372749695, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4124684372749695\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.37512893628136457, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.37512893628136457\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5882322650485803, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5882322650485803\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8262199992945417, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8262199992945417\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4124684372749695, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4124684372749695\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.37512893628136457, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.37512893628136457\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5882322650485803, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5882322650485803\n",
      "[LightGBM] [Info] Number of positive: 35520, number of negative: 2412919\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.219216 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8482\n",
      "[LightGBM] [Info] Number of data points in the train set: 2448439, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014507 -> initscore=-4.218497\n",
      "[LightGBM] [Info] Start training from score -4.218497\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8262199992945417, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8262199992945417\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4124684372749695, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4124684372749695\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.37512893628136457, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.37512893628136457\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5882322650485803, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5882322650485803\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8633117565583335, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8633117565583335\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.55290923003859, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.55290923003859\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3492214895079347, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3492214895079347\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8505357588039386, subsample=0.9543648555046881 will be ignored. Current value: bagging_fraction=0.8505357588039386\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8633117565583335, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8633117565583335\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.55290923003859, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.55290923003859\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3492214895079347, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3492214895079347\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8505357588039386, subsample=0.9543648555046881 will be ignored. Current value: bagging_fraction=0.8505357588039386\n",
      "[LightGBM] [Info] Number of positive: 2401442, number of negative: 2401442\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.475835 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8277\n",
      "[LightGBM] [Info] Number of data points in the train set: 4802884, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8633117565583335, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8633117565583335\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.55290923003859, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.55290923003859\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3492214895079347, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3492214895079347\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8505357588039386, subsample=0.9543648555046881 will be ignored. Current value: bagging_fraction=0.8505357588039386\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   discarded       0.85      0.70      0.77    333038\n",
      "    question       0.77      0.87      0.82    315240\n",
      "     section       0.26      0.80      0.39     12637\n",
      "  subsection       0.88      0.02      0.05      9856\n",
      "\n",
      "    accuracy                           0.78    670771\n",
      "   macro avg       0.69      0.60      0.51    670771\n",
      "weighted avg       0.80      0.78      0.78    670771\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   discarded       0.85      0.70      0.77    333038\n",
      "    question       0.77      0.87      0.82    315240\n",
      "     section       0.26      0.80      0.39     12637\n",
      "  subsection       0.88      0.02      0.05      9856\n",
      "\n",
      "    accuracy                           0.78    670771\n",
      "   macro avg       0.69      0.60      0.51    670771\n",
      "weighted avg       0.80      0.78      0.78    670771\n",
      "\n",
      "run 4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.820049231809358, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.820049231809358\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.35398913607890153, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.35398913607890153\n",
      "[LightGBM] [Warning] lambda_l2 is set=2.413763938888623, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.413763938888623\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7939521214604877, subsample=0.3170686952817686 will be ignored. Current value: bagging_fraction=0.7939521214604877\n",
      "[LightGBM] [Warning] feature_fraction is set=0.820049231809358, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.820049231809358\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.35398913607890153, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.35398913607890153\n",
      "[LightGBM] [Warning] lambda_l2 is set=2.413763938888623, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.413763938888623\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7939521214604877, subsample=0.3170686952817686 will be ignored. Current value: bagging_fraction=0.7939521214604877\n",
      "[LightGBM] [Info] Number of positive: 1141432, number of negative: 1303286\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.240339 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8427\n",
      "[LightGBM] [Info] Number of data points in the train set: 2444718, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.466897 -> initscore=-0.132605\n",
      "[LightGBM] [Info] Start training from score -0.132605\n",
      "[LightGBM] [Warning] feature_fraction is set=0.820049231809358, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.820049231809358\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.35398913607890153, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.35398913607890153\n",
      "[LightGBM] [Warning] lambda_l2 is set=2.413763938888623, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.413763938888623\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7939521214604877, subsample=0.3170686952817686 will be ignored. Current value: bagging_fraction=0.7939521214604877\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] lambda_l1 is set=9.82085679893001, reg_alpha=0.0 will be ignored. Current value: lambda_l1=9.82085679893001\n",
      "[LightGBM] [Warning] lambda_l2 is set=7.043665039579396, reg_lambda=0.0 will be ignored. Current value: lambda_l2=7.043665039579396\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] lambda_l1 is set=9.82085679893001, reg_alpha=0.0 will be ignored. Current value: lambda_l1=9.82085679893001\n",
      "[LightGBM] [Warning] lambda_l2 is set=7.043665039579396, reg_lambda=0.0 will be ignored. Current value: lambda_l2=7.043665039579396\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Info] Number of positive: 1220374, number of negative: 1220374\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.241069 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8351\n",
      "[LightGBM] [Info] Number of data points in the train set: 2440748, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] lambda_l1 is set=9.82085679893001, reg_alpha=0.0 will be ignored. Current value: lambda_l1=9.82085679893001\n",
      "[LightGBM] [Warning] lambda_l2 is set=7.043665039579396, reg_lambda=0.0 will be ignored. Current value: lambda_l2=7.043665039579396\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8262199992945417, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8262199992945417\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4124684372749695, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4124684372749695\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.37512893628136457, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.37512893628136457\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5882322650485803, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5882322650485803\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8262199992945417, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8262199992945417\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4124684372749695, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4124684372749695\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.37512893628136457, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.37512893628136457\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5882322650485803, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5882322650485803\n",
      "[LightGBM] [Info] Number of positive: 35662, number of negative: 2409056\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.216010 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8427\n",
      "[LightGBM] [Info] Number of data points in the train set: 2444718, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014587 -> initscore=-4.212905\n",
      "[LightGBM] [Info] Start training from score -4.212905\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8262199992945417, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8262199992945417\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4124684372749695, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4124684372749695\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.37512893628136457, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.37512893628136457\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5882322650485803, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5882322650485803\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8633117565583335, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8633117565583335\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.55290923003859, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.55290923003859\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3492214895079347, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3492214895079347\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8505357588039386, subsample=0.9543648555046881 will be ignored. Current value: bagging_fraction=0.8505357588039386\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8633117565583335, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8633117565583335\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.55290923003859, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.55290923003859\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3492214895079347, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3492214895079347\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8505357588039386, subsample=0.9543648555046881 will be ignored. Current value: bagging_fraction=0.8505357588039386\n",
      "[LightGBM] [Info] Number of positive: 2397468, number of negative: 2397468\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.453323 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8101\n",
      "[LightGBM] [Info] Number of data points in the train set: 4794936, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8633117565583335, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8633117565583335\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.55290923003859, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.55290923003859\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3492214895079347, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3492214895079347\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8505357588039386, subsample=0.9543648555046881 will be ignored. Current value: bagging_fraction=0.8505357588039386\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   discarded       0.86      0.72      0.78    344938\n",
      "    question       0.77      0.87      0.82    307456\n",
      "     section       0.25      0.79      0.38     12384\n",
      "  subsection       0.86      0.02      0.04      9714\n",
      "\n",
      "    accuracy                           0.78    674492\n",
      "   macro avg       0.69      0.60      0.51    674492\n",
      "weighted avg       0.81      0.78      0.78    674492\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   discarded       0.86      0.72      0.78    344938\n",
      "    question       0.77      0.87      0.82    307456\n",
      "     section       0.25      0.79      0.38     12384\n",
      "  subsection       0.86      0.02      0.04      9714\n",
      "\n",
      "    accuracy                           0.78    674492\n",
      "   macro avg       0.69      0.60      0.51    674492\n",
      "weighted avg       0.81      0.78      0.78    674492\n",
      "\n",
      "run 5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.820049231809358, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.820049231809358\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.35398913607890153, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.35398913607890153\n",
      "[LightGBM] [Warning] lambda_l2 is set=2.413763938888623, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.413763938888623\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7939521214604877, subsample=0.3170686952817686 will be ignored. Current value: bagging_fraction=0.7939521214604877\n",
      "[LightGBM] [Warning] feature_fraction is set=0.820049231809358, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.820049231809358\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.35398913607890153, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.35398913607890153\n",
      "[LightGBM] [Warning] lambda_l2 is set=2.413763938888623, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.413763938888623\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7939521214604877, subsample=0.3170686952817686 will be ignored. Current value: bagging_fraction=0.7939521214604877\n",
      "[LightGBM] [Info] Number of positive: 1130426, number of negative: 1309641\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.233014 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8463\n",
      "[LightGBM] [Info] Number of data points in the train set: 2440067, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.463277 -> initscore=-0.147159\n",
      "[LightGBM] [Info] Start training from score -0.147159\n",
      "[LightGBM] [Warning] feature_fraction is set=0.820049231809358, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.820049231809358\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.35398913607890153, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.35398913607890153\n",
      "[LightGBM] [Warning] lambda_l2 is set=2.413763938888623, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.413763938888623\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7939521214604877, subsample=0.3170686952817686 will be ignored. Current value: bagging_fraction=0.7939521214604877\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] lambda_l1 is set=9.82085679893001, reg_alpha=0.0 will be ignored. Current value: lambda_l1=9.82085679893001\n",
      "[LightGBM] [Warning] lambda_l2 is set=7.043665039579396, reg_lambda=0.0 will be ignored. Current value: lambda_l2=7.043665039579396\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] lambda_l1 is set=9.82085679893001, reg_alpha=0.0 will be ignored. Current value: lambda_l1=9.82085679893001\n",
      "[LightGBM] [Warning] lambda_l2 is set=7.043665039579396, reg_lambda=0.0 will be ignored. Current value: lambda_l2=7.043665039579396\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Info] Number of positive: 1212797, number of negative: 1212797\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.252778 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8457\n",
      "[LightGBM] [Info] Number of data points in the train set: 2425594, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] lambda_l1 is set=9.82085679893001, reg_alpha=0.0 will be ignored. Current value: lambda_l1=9.82085679893001\n",
      "[LightGBM] [Warning] lambda_l2 is set=7.043665039579396, reg_lambda=0.0 will be ignored. Current value: lambda_l2=7.043665039579396\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8262199992945417, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8262199992945417\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4124684372749695, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4124684372749695\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.37512893628136457, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.37512893628136457\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5882322650485803, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5882322650485803\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8262199992945417, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8262199992945417\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4124684372749695, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4124684372749695\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.37512893628136457, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.37512893628136457\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5882322650485803, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5882322650485803\n",
      "[LightGBM] [Info] Number of positive: 35898, number of negative: 2404169\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.216050 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8463\n",
      "[LightGBM] [Info] Number of data points in the train set: 2440067, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014712 -> initscore=-4.204278\n",
      "[LightGBM] [Info] Start training from score -4.204278\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8262199992945417, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8262199992945417\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.4124684372749695, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.4124684372749695\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.37512893628136457, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.37512893628136457\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5882322650485803, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5882322650485803\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8633117565583335, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8633117565583335\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.55290923003859, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.55290923003859\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3492214895079347, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3492214895079347\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8505357588039386, subsample=0.9543648555046881 will be ignored. Current value: bagging_fraction=0.8505357588039386\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8633117565583335, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8633117565583335\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.55290923003859, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.55290923003859\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3492214895079347, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3492214895079347\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8505357588039386, subsample=0.9543648555046881 will be ignored. Current value: bagging_fraction=0.8505357588039386\n",
      "[LightGBM] [Info] Number of positive: 2393594, number of negative: 2393594\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.477271 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8104\n",
      "[LightGBM] [Info] Number of data points in the train set: 4787188, number of used features: 112\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8633117565583335, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8633117565583335\n",
      "[LightGBM] [Warning] lambda_l1 is set=7.55290923003859, reg_alpha=0.0 will be ignored. Current value: lambda_l1=7.55290923003859\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.3492214895079347, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.3492214895079347\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8505357588039386, subsample=0.9543648555046881 will be ignored. Current value: bagging_fraction=0.8505357588039386\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   discarded       0.86      0.72      0.78    338042\n",
      "    question       0.78      0.88      0.83    318462\n",
      "     section       0.26      0.79      0.39     13161\n",
      "  subsection       0.85      0.01      0.02      9478\n",
      "\n",
      "    accuracy                           0.79    679143\n",
      "   macro avg       0.69      0.60      0.51    679143\n",
      "weighted avg       0.81      0.79      0.79    679143\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   discarded       0.86      0.72      0.78    338042\n",
      "    question       0.78      0.88      0.83    318462\n",
      "     section       0.26      0.79      0.39     13161\n",
      "  subsection       0.85      0.01      0.02      9478\n",
      "\n",
      "    accuracy                           0.79    679143\n",
      "   macro avg       0.69      0.60      0.51    679143\n",
      "weighted avg       0.81      0.79      0.79    679143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reports4 = []\n",
    "\n",
    "for r in r_state:\n",
    "    print(f'run {r}')\n",
    "    train_df = train_df_dict[r]\n",
    "    test_df = test_df_dict[r]\n",
    "    z = multi_pred_stats([train_test('question','none',params_q)[0],train_test('discarded','under',params_d)[0],train_test('subsection','none',params_ss)[0],train_test('section','over',params_s)[0]])\n",
    "    reports4.append(z)\n",
    "    print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2810a6a5-a8a5-4ac9-b7a0-2b7d7b5b87f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   discarded       0.86      0.70      0.77    343880\n",
      "    question       0.76      0.87      0.82    313673\n",
      "     section       0.25      0.80      0.38     12516\n",
      "  subsection       0.70      0.01      0.01     10212\n",
      "\n",
      "    accuracy                           0.77    680281\n",
      "   macro avg       0.64      0.60      0.50    680281\n",
      "weighted avg       0.80      0.77      0.77    680281\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   discarded       0.86      0.72      0.78    330279\n",
      "    question       0.78      0.88      0.83    310293\n",
      "     section       0.26      0.79      0.39     12745\n",
      "  subsection       0.81      0.01      0.02      9174\n",
      "\n",
      "    accuracy                           0.78    662491\n",
      "   macro avg       0.68      0.60      0.50    662491\n",
      "weighted avg       0.81      0.78      0.79    662491\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   discarded       0.85      0.70      0.77    333038\n",
      "    question       0.77      0.87      0.82    315240\n",
      "     section       0.26      0.80      0.39     12637\n",
      "  subsection       0.88      0.02      0.05      9856\n",
      "\n",
      "    accuracy                           0.78    670771\n",
      "   macro avg       0.69      0.60      0.51    670771\n",
      "weighted avg       0.80      0.78      0.78    670771\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   discarded       0.86      0.72      0.78    344938\n",
      "    question       0.77      0.87      0.82    307456\n",
      "     section       0.25      0.79      0.38     12384\n",
      "  subsection       0.86      0.02      0.04      9714\n",
      "\n",
      "    accuracy                           0.78    674492\n",
      "   macro avg       0.69      0.60      0.51    674492\n",
      "weighted avg       0.81      0.78      0.78    674492\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   discarded       0.86      0.72      0.78    338042\n",
      "    question       0.78      0.88      0.83    318462\n",
      "     section       0.26      0.79      0.39     13161\n",
      "  subsection       0.85      0.01      0.02      9478\n",
      "\n",
      "    accuracy                           0.79    679143\n",
      "   macro avg       0.69      0.60      0.51    679143\n",
      "weighted avg       0.81      0.79      0.79    679143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for res in reports4:\n",
    "    print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
